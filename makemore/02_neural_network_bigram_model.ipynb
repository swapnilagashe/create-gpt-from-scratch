{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt','r').read().splitlines()\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are wasting some space as one row and one column is completely 0\n",
    "# we don't need to use both start and end tokens, we will use just one '.' \n",
    "\n",
    "N = torch.zeros((27,27), dtype=torch.int32)\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.']= 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,ys =[],[]\n",
    "\n",
    "for w in words[:1]: # taken only one word\n",
    "    chs = ['.']+list(w)+['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1],stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs) #torch.Tensor(), capital T is different, torch.Tensor() return dtype as float32, lowercase infers the datatype (int in this case)\n",
    "ys = torch.tensor(ys)# recommended to always use torch.tensor and not torch.Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 13, 13,  1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys # for input 0, output is 5, for input 5 output is 13 and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since we have a single integer as an example, we need to encode them - using one hot encoding\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # now these vectors can be put in the NN, we want dtype to be float and not int\n",
    "xenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 27]),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x16d774110>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27, 27])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.3458e-02,  1.6452e+00,  1.7143e+00, -1.6903e+00,  1.0903e+00,\n",
       "         -4.4693e-01,  9.3233e-01,  1.0034e+00, -4.9965e-01, -9.8683e-01,\n",
       "          6.6400e-01,  8.1470e-01, -1.4223e+00, -1.4988e+00, -1.7541e+00,\n",
       "         -3.1863e-01,  2.9994e-01, -7.1740e-01, -3.1648e-01,  3.2639e-01,\n",
       "         -1.1404e+00,  6.6917e-01,  1.1645e+00, -3.7464e-01,  2.7272e-01,\n",
       "         -3.3692e-01, -7.2400e-01],\n",
       "        [-4.3819e-01,  1.9125e+00,  1.4317e+00,  1.4990e+00,  2.0811e+00,\n",
       "          1.7368e-01, -1.9796e-01, -7.9720e-01, -8.1439e-01,  2.2169e-01,\n",
       "          1.6824e+00, -7.5749e-01, -2.8739e-01,  1.9046e+00, -3.2517e-01,\n",
       "         -1.6542e+00, -9.5115e-02,  1.0136e+00, -3.9064e-01,  1.7885e+00,\n",
       "          4.0772e-01,  2.1609e+00,  3.0456e-02, -1.3655e-01,  1.6446e+00,\n",
       "          1.3668e+00, -4.5439e-02],\n",
       "        [-1.5792e-02,  1.8244e+00, -6.4208e-01, -1.2195e+00, -4.9707e-01,\n",
       "          9.6834e-01,  1.5457e-01,  2.6740e+00,  2.0278e+00, -2.2920e+00,\n",
       "          2.3528e-01, -2.7620e+00, -1.8784e+00,  8.0645e-01, -1.2123e+00,\n",
       "          6.9127e-01, -4.8532e-01,  2.2606e-01, -2.7049e-01, -1.0297e+00,\n",
       "         -1.4982e+00, -2.2019e+00,  1.8059e+00, -3.5504e-01,  5.2373e-01,\n",
       "         -3.5752e-01, -2.8104e-01],\n",
       "        [-1.2436e-01, -1.1953e+00, -6.3054e-01, -1.1518e-02,  3.2575e-01,\n",
       "          4.3729e-01, -1.9897e+00,  4.3494e-01, -7.3684e-01, -6.2716e-01,\n",
       "         -1.2225e-01,  1.6652e-01, -7.4095e-01,  3.8712e-01, -4.3071e-01,\n",
       "         -1.6432e+00, -6.3376e-02, -3.0400e-01,  2.8874e-01,  9.4593e-01,\n",
       "          1.3712e+00, -2.0901e-01,  2.1256e+00, -5.2055e-02,  9.1777e-01,\n",
       "          1.6704e+00,  3.4556e-01],\n",
       "        [-7.3379e-01,  3.3225e-01, -5.4672e-01, -4.8100e-01, -1.3768e-01,\n",
       "          2.2035e-01,  7.5762e-01,  1.0134e+00, -9.6739e-01, -5.4132e-01,\n",
       "          3.7778e-01,  2.4239e-01, -2.3881e+00,  1.0070e+00, -1.0346e-01,\n",
       "         -6.4071e-01,  1.3293e-01, -1.9921e-02, -1.0812e+00, -9.8085e-01,\n",
       "         -8.3770e-01, -8.6756e-01, -1.7814e+00,  1.9540e+00, -1.5068e+00,\n",
       "          2.3449e+00, -2.4708e-01],\n",
       "        [ 4.7124e-01,  5.4257e-02, -7.4907e-01, -1.2211e+00, -1.2639e+00,\n",
       "         -8.6128e-01,  1.7420e+00, -7.6527e-01, -4.7898e-01,  5.8289e-01,\n",
       "         -4.8730e-01,  1.1558e+00, -4.6673e-01,  4.1638e-01,  1.9125e+00,\n",
       "          2.8942e-01,  4.4042e-01,  8.7235e-01,  1.6236e-01, -9.1069e-01,\n",
       "         -1.0327e+00,  1.4922e+00, -1.1040e+00,  2.5726e-02,  1.3004e+00,\n",
       "          1.7106e+00, -5.0516e-01],\n",
       "        [-5.2771e-01, -2.7605e-01,  8.1137e-01, -1.7752e+00, -4.0894e-01,\n",
       "          1.0841e+00,  4.0586e-01, -2.4617e-01,  4.1926e-01,  6.3437e-01,\n",
       "          6.9672e-01, -1.1067e+00,  4.9250e-01,  1.6787e+00,  3.1951e-01,\n",
       "         -1.0282e+00, -2.7185e-01, -1.2714e+00,  1.1210e+00, -3.0072e-01,\n",
       "         -2.3073e-01,  2.8213e-01, -5.5632e-01, -9.4563e-01,  1.6660e-01,\n",
       "         -1.8855e-01, -5.1101e-01],\n",
       "        [-1.3150e+00, -1.1141e+00, -1.1985e+00,  1.4405e-01, -1.0206e-01,\n",
       "          1.1149e+00,  1.9493e+00, -3.2819e-01, -9.4841e-01,  3.5341e-02,\n",
       "         -1.0282e+00,  1.1588e+00, -1.0395e+00,  6.0997e-01, -1.2501e+00,\n",
       "         -5.0061e-02, -1.2473e+00,  1.7428e+00, -1.5639e+00,  2.0825e+00,\n",
       "         -3.4444e-01,  9.4276e-01, -2.6173e-01,  2.8716e-01, -2.9824e-01,\n",
       "         -1.0624e+00, -3.2488e-01],\n",
       "        [-9.9358e-01,  8.4214e-01, -2.7124e-01,  1.3916e+00, -6.0623e-01,\n",
       "          2.6580e-01, -5.0142e-01,  1.4677e+00,  5.6331e-01, -1.1718e+00,\n",
       "          2.2007e+00, -5.7761e-01,  1.1846e+00,  9.8132e-02, -3.2773e-01,\n",
       "          3.0904e-01,  5.0269e-01,  7.3794e-01, -1.6998e+00, -1.5526e-01,\n",
       "         -1.7518e-02,  6.6170e-02,  2.2445e+00, -1.1251e+00, -2.6821e-01,\n",
       "         -7.5277e-01, -7.3134e-01],\n",
       "        [-1.9721e+00,  5.5576e-01,  1.1053e+00, -9.7873e-01, -1.9033e+00,\n",
       "         -7.3707e-02, -2.5708e-01, -6.4516e-02,  1.4067e+00, -1.3290e+00,\n",
       "          9.8271e-01, -1.3650e+00,  1.5025e+00, -2.0534e-01, -2.2203e+00,\n",
       "          8.1995e-01, -3.2745e-01, -3.4432e-02,  2.3080e-01,  2.8994e-02,\n",
       "          9.0339e-02, -5.5014e-01,  3.5940e-01,  6.3540e-01, -6.4189e-02,\n",
       "         -2.2593e+00, -2.8021e-01],\n",
       "        [-1.1861e+00, -1.0393e-01,  1.0167e+00, -1.4605e+00, -3.1862e-01,\n",
       "         -5.8182e-01, -7.5177e-01,  3.3597e-01,  1.7313e+00,  1.3194e+00,\n",
       "         -4.3742e-01,  2.3648e+00, -1.6230e+00,  1.0494e-01, -1.2907e+00,\n",
       "         -1.9142e-01, -3.2565e-01, -6.0454e-01, -2.0791e-01,  6.6005e-01,\n",
       "          8.1240e-01, -2.6874e+00, -9.2902e-01, -1.3324e+00, -1.8611e-01,\n",
       "          2.2058e+00,  6.3332e-01],\n",
       "        [ 6.1415e-01, -9.1948e-01, -3.7366e-01,  1.3225e+00,  6.8645e-01,\n",
       "         -6.4245e-01, -1.5739e-01,  5.4202e-01,  5.0011e-01,  2.7362e-01,\n",
       "          1.9695e-01,  1.8117e-03,  5.7041e-01, -1.1504e+00, -1.9215e-01,\n",
       "          1.7223e+00,  6.0581e-01, -6.1279e-02, -2.3252e-01,  6.0834e-01,\n",
       "         -2.7900e-03, -9.2474e-01, -2.1601e+00, -3.4101e-01,  1.3518e+00,\n",
       "          6.5569e-01,  1.8581e+00],\n",
       "        [-7.7239e-01,  1.4866e+00, -9.3698e-01, -6.4812e-01,  2.5924e-01,\n",
       "         -8.4128e-01, -8.4711e-01, -1.5055e+00,  8.8055e-01, -1.7235e+00,\n",
       "         -8.9954e-01,  1.1021e+00,  9.6818e-01,  4.1030e-01, -2.3074e-01,\n",
       "          3.8908e-01, -1.9143e+00, -1.0728e+00, -9.5507e-02, -1.0695e+00,\n",
       "          1.4459e-01,  6.9251e-01,  5.4014e-01,  1.1845e+00, -1.0956e-01,\n",
       "         -8.6497e-01,  2.4320e-01],\n",
       "        [ 1.5446e+00, -2.7666e-01, -4.9097e-01, -2.5803e-01,  1.9286e+00,\n",
       "          8.1600e-01, -1.3986e+00,  6.6020e-01,  2.5341e+00,  2.2178e-01,\n",
       "          9.6466e-01, -6.2989e-01,  5.0626e-02,  3.3362e-01,  5.2196e-01,\n",
       "         -1.7737e+00, -1.0262e+00, -2.0984e+00, -7.6658e-01, -9.9579e-01,\n",
       "         -1.0026e+00, -3.5183e-01, -3.9399e-01,  1.5004e-01,  1.0975e+00,\n",
       "          4.1378e-01, -1.5800e+00],\n",
       "        [ 4.0939e-01,  4.2978e-01, -2.8363e-01,  1.0181e+00,  9.7872e-02,\n",
       "          3.0596e-01, -4.5809e-01, -8.5218e-01,  4.6249e-01,  8.0492e-01,\n",
       "         -2.1291e-01, -1.3788e+00, -1.2169e-01, -1.8221e+00,  9.8518e-01,\n",
       "         -2.1010e-01,  2.7923e-01,  5.2872e-01, -3.2850e-02, -3.9633e-02,\n",
       "         -1.3549e+00,  3.6553e-01,  6.3290e-01, -2.6470e-01,  8.8408e-01,\n",
       "          5.2686e-01,  2.1797e+00],\n",
       "        [-7.8828e-01,  9.9192e-01,  8.5440e-01,  9.0618e-01,  1.2877e-01,\n",
       "         -1.6623e+00, -2.1875e+00,  1.1714e+00, -6.2914e-01, -8.5155e-01,\n",
       "          5.5129e-01, -1.9434e-01,  3.0421e-01, -1.2227e+00, -3.7003e-01,\n",
       "         -9.3158e-01, -5.9132e-01, -7.3849e-01, -1.5059e-01, -2.3490e-01,\n",
       "          6.3505e-01, -1.5897e+00,  5.5657e-01, -1.7946e+00,  1.1720e-01,\n",
       "         -1.2751e+00, -4.3181e-01],\n",
       "        [ 2.6023e-02,  1.8541e+00, -5.5848e-01, -4.6571e-01,  1.1284e+00,\n",
       "         -5.2808e-01,  9.5297e-01, -1.0456e+00, -4.9008e-01,  1.1140e+00,\n",
       "          1.4558e+00,  4.8872e-01,  2.4736e-01,  7.5285e-01, -4.1470e-01,\n",
       "         -1.1077e+00,  2.5911e-01,  8.0948e-01,  1.7165e+00,  2.1315e+00,\n",
       "          1.0624e+00,  1.6987e-01,  3.1543e-01, -4.1859e-01, -1.3231e+00,\n",
       "          1.2567e+00, -2.2082e-01],\n",
       "        [-1.5826e-01, -9.9668e-01, -6.3699e-01,  7.2845e-01, -5.7540e-01,\n",
       "          9.8638e-01, -9.6523e-01, -2.2725e-01, -5.3339e-01, -4.2858e-01,\n",
       "          9.6689e-01,  7.2165e-01, -5.3620e-01, -1.1598e+00,  5.9742e-01,\n",
       "         -7.9910e-01, -6.0416e-01, -1.5216e+00, -6.9938e-01, -2.8236e+00,\n",
       "          6.5975e-01,  1.3640e+00, -8.4718e-01,  2.1026e+00,  7.3667e-01,\n",
       "         -1.1822e+00,  2.3735e+00],\n",
       "        [-1.0900e+00,  1.0467e+00, -1.5444e+00, -1.9945e+00,  4.6198e-01,\n",
       "         -1.4271e+00, -9.4374e-02, -1.3400e+00,  8.1073e-01,  6.9161e-01,\n",
       "         -6.1970e-01,  1.6346e+00, -1.7342e-01, -5.5853e-01, -1.2392e+00,\n",
       "          6.3131e-01, -1.0236e+00, -1.2923e+00, -8.3690e-02, -9.1119e-01,\n",
       "          2.7313e+00, -1.5150e+00, -2.2974e-01, -8.3335e-01, -5.4455e-01,\n",
       "         -1.2551e+00, -9.2396e-01],\n",
       "        [-6.3751e-01, -1.0685e+00,  5.5012e-01,  8.7432e-01,  1.9512e+00,\n",
       "          1.2660e+00,  5.6449e-01,  8.8919e-01, -1.5371e+00,  9.2932e-01,\n",
       "          1.0528e+00,  4.9545e-01,  7.0088e-01, -2.2228e-01, -1.4261e-01,\n",
       "         -5.6126e-01,  1.1839e+00,  1.3787e-01, -1.7592e+00,  1.2619e+00,\n",
       "         -8.4649e-01, -1.7921e-01, -9.8656e-01, -1.1592e+00,  4.9833e-01,\n",
       "          1.2479e+00,  7.4090e-01],\n",
       "        [ 1.1455e+00,  1.8676e-01, -4.3938e-01,  1.8454e+00,  3.5635e-01,\n",
       "         -1.0249e+00, -1.6107e-01,  2.2000e+00,  7.3414e-01, -2.2384e-01,\n",
       "          1.4084e+00, -1.3361e+00,  1.2003e+00,  4.3026e-01, -1.1328e+00,\n",
       "         -9.6006e-01, -1.0468e+00, -5.0273e-01,  8.0113e-01, -1.2115e+00,\n",
       "         -8.1151e-01,  5.1101e-02, -1.2128e+00, -1.9634e+00, -5.3232e-01,\n",
       "         -2.8525e-01,  1.1350e+00],\n",
       "        [-3.1736e-02,  1.9813e+00, -1.3784e+00, -9.6055e-01,  7.8473e-01,\n",
       "          7.2767e-01,  1.6505e+00, -8.1535e-01, -1.1435e+00,  3.3026e-01,\n",
       "         -7.4727e-01, -1.3403e+00,  8.8537e-01, -7.1644e-01,  2.8996e-01,\n",
       "          3.6791e-01, -1.3013e+00, -1.2674e-01, -1.2054e+00, -2.2052e+00,\n",
       "         -7.5917e-01, -2.0796e-01,  1.1461e+00, -3.1833e-01,  1.3851e+00,\n",
       "          6.6554e-01, -1.8049e+00],\n",
       "        [ 2.4594e-01, -2.3730e-01,  1.8214e+00,  8.7388e-01, -9.1251e-01,\n",
       "          1.1366e+00,  1.5830e+00, -5.8769e-01, -5.7793e-02,  1.2628e+00,\n",
       "          2.6147e-01, -1.5639e+00,  1.7782e+00,  5.1977e-02,  1.3171e+00,\n",
       "         -7.3286e-01, -1.4589e+00, -5.8665e-01, -7.8064e-01, -1.8391e+00,\n",
       "         -1.5142e-01, -1.4365e-01, -2.8707e-01,  2.2197e+00, -3.2004e-01,\n",
       "          1.4231e-01,  1.1901e+00],\n",
       "        [ 1.2239e+00, -1.2769e+00,  9.6501e-01,  1.8864e+00,  5.4164e-01,\n",
       "         -3.1264e-02, -3.5344e-02,  1.1836e+00, -3.4459e-01, -6.9655e-01,\n",
       "         -6.4615e-02,  9.2732e-01, -1.2633e+00,  6.1341e-01, -7.8061e-01,\n",
       "         -2.2336e-01,  1.1209e+00, -5.4279e-02,  7.1969e-01, -1.3745e+00,\n",
       "         -1.4312e-01, -9.9512e-01, -1.1562e-01,  5.0855e-01, -2.5605e-01,\n",
       "         -7.5838e-01, -4.1053e-01],\n",
       "        [-1.0206e+00, -2.4535e+00, -1.0018e+00, -7.7907e-01,  3.3545e-01,\n",
       "         -1.3084e+00, -8.9910e-01,  6.4865e-01, -5.7586e-01, -3.7223e-01,\n",
       "         -8.6778e-01,  9.3928e-01, -7.4552e-01, -7.7688e-01, -8.6843e-01,\n",
       "          1.1628e+00, -6.5061e-01, -2.7531e+00,  8.7691e-02, -1.0954e+00,\n",
       "         -4.2429e-01, -1.0831e+00,  1.0286e+00, -1.1780e+00,  1.0491e+00,\n",
       "         -9.0166e-01,  3.2634e-01],\n",
       "        [-3.1160e-01, -8.3321e-01,  6.3237e-01, -7.7683e-01, -5.7829e-01,\n",
       "          7.8064e-01, -2.0380e-02,  8.1379e-01,  9.7870e-01,  1.4545e-02,\n",
       "          5.4561e-01, -1.1310e+00, -7.7356e-01, -3.5138e+00, -4.2742e-02,\n",
       "         -1.2153e+00,  8.5246e-01,  1.7181e+00, -6.2345e-01,  1.2454e+00,\n",
       "          5.2671e-01,  5.4215e-01,  1.0467e+00, -9.2360e-01,  1.8860e-01,\n",
       "          4.1726e-01,  1.7273e+00],\n",
       "        [-4.5627e-02, -3.8387e-02,  1.0029e-01,  2.0521e-01, -6.6328e-01,\n",
       "         -4.0185e-01, -1.1554e+00, -2.7911e-01,  2.0672e+00,  4.9127e-01,\n",
       "          6.8900e-01,  3.6268e-01, -4.5551e-01, -2.0472e-01,  2.8120e-01,\n",
       "          7.4619e-01,  2.8777e-01, -1.4445e-01,  1.3521e+00, -1.1421e+00,\n",
       "          4.6596e-01, -1.4761e+00,  1.6696e+00,  1.4244e-01, -2.5232e+00,\n",
       "         -6.2212e-01,  2.1870e-01]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.randn((27,27))\n",
    "print(W.shape)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 27]), torch.Size([27, 27]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.shape, W.shape # multiplying these two will return 5X1 tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0435,  1.6452,  1.7143, -1.6903,  1.0903, -0.4469,  0.9323,  1.0034,\n",
       "         -0.4997, -0.9868,  0.6640,  0.8147, -1.4223, -1.4988, -1.7541, -0.3186,\n",
       "          0.2999, -0.7174, -0.3165,  0.3264, -1.1404,  0.6692,  1.1645, -0.3746,\n",
       "          0.2727, -0.3369, -0.7240],\n",
       "        [ 0.4712,  0.0543, -0.7491, -1.2211, -1.2639, -0.8613,  1.7420, -0.7653,\n",
       "         -0.4790,  0.5829, -0.4873,  1.1558, -0.4667,  0.4164,  1.9125,  0.2894,\n",
       "          0.4404,  0.8723,  0.1624, -0.9107, -1.0327,  1.4922, -1.1040,  0.0257,\n",
       "          1.3004,  1.7106, -0.5052],\n",
       "        [ 1.5446, -0.2767, -0.4910, -0.2580,  1.9286,  0.8160, -1.3986,  0.6602,\n",
       "          2.5341,  0.2218,  0.9647, -0.6299,  0.0506,  0.3336,  0.5220, -1.7737,\n",
       "         -1.0262, -2.0984, -0.7666, -0.9958, -1.0026, -0.3518, -0.3940,  0.1500,\n",
       "          1.0975,  0.4138, -1.5800],\n",
       "        [ 1.5446, -0.2767, -0.4910, -0.2580,  1.9286,  0.8160, -1.3986,  0.6602,\n",
       "          2.5341,  0.2218,  0.9647, -0.6299,  0.0506,  0.3336,  0.5220, -1.7737,\n",
       "         -1.0262, -2.0984, -0.7666, -0.9958, -1.0026, -0.3518, -0.3940,  0.1500,\n",
       "          1.0975,  0.4138, -1.5800],\n",
       "        [-0.4382,  1.9125,  1.4317,  1.4990,  2.0811,  0.1737, -0.1980, -0.7972,\n",
       "         -0.8144,  0.2217,  1.6824, -0.7575, -0.2874,  1.9046, -0.3252, -1.6542,\n",
       "         -0.0951,  1.0136, -0.3906,  1.7885,  0.4077,  2.1609,  0.0305, -0.1365,\n",
       "          1.6446,  1.3668, -0.0454]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc @ W # dot product of xenc and W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3336)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(xenc @ W)[3,13] # this gives the firing rate of the 13th neuron when 3rd example is the input\n",
    "# this was achieved by a dot product between the 3rd input and the 13th column of the W matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3336)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aboeve is equivalent to\n",
    "(xenc[3]*W[:,13]).sum() # but pytorch does it parallely and efficiently using the above process for a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly initialise 27 neurons' weights. each neuron receives 27 inputs\n",
    "# using a generator just so if anyone uses the code, they get same W initilisation\n",
    "\n",
    "# initialising weights\n",
    "g= torch.Generator().manual_seed(42)\n",
    "W = torch.randn((27,27),generator=g) \n",
    "\n",
    "#forward pass\n",
    "# encoding inputs\n",
    "xenc = F.one_hot(xs, num_classes=27).float() # now these vectors can be put in the NN, we want dtype to be float and not int\n",
    "\n",
    "logits = xenc @ W # predict log counts\n",
    "\n",
    "# applying softmax to output the probability distributions\n",
    "counts = logits.exp() # equilvalent to N used in previous notebook\n",
    "probs = counts/counts.sum(1,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(True), torch.Size([5, 27]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum() == 1, probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "bigram example 1: .e (indexes 0,5)\n",
      "input to the neural net: 0\n",
      "output probabilities from the neural net: tensor([0.1230, 0.0793, 0.0441, 0.0022, 0.0353, 0.0052, 0.0172, 0.0036, 0.0084,\n",
      "        0.0932, 0.0121, 0.0044, 0.0087, 0.0102, 0.0083, 0.0384, 0.0926, 0.0153,\n",
      "        0.0109, 0.0278, 0.0084, 0.0527, 0.0399, 0.0962, 0.0644, 0.0655, 0.0330])\n",
      "label (actual next character): 5\n",
      "probability assigned by the the net tothe correct char: 0.005211981013417244\n",
      "log likelihood: -5.256795406341553\n",
      "Negative log likelihood: 5.256795406341553\n",
      "----------------------------------------------------------------\n",
      "bigram example 2: em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0396, 0.0698, 0.0227, 0.0037, 0.0562, 0.0153, 0.0626, 0.0112, 0.0421,\n",
      "        0.0084, 0.0581, 0.1125, 0.0951, 0.0292, 0.0181, 0.0107, 0.0247, 0.0316,\n",
      "        0.0586, 0.0140, 0.1109, 0.0019, 0.0147, 0.0067, 0.0502, 0.0033, 0.0280])\n",
      "label (actual next character): 13\n",
      "probability assigned by the the net tothe correct char: 0.029175996780395508\n",
      "log likelihood: -3.5344090461730957\n",
      "Negative log likelihood: 3.5344090461730957\n",
      "----------------------------------------------------------------\n",
      "bigram example 3: mm (indexes 13,13)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0123, 0.0078, 0.0250, 0.0187, 0.0050, 0.0125, 0.0074, 0.0099, 0.1254,\n",
      "        0.1147, 0.0934, 0.0176, 0.0144, 0.0029, 0.0085, 0.0116, 0.1132, 0.0138,\n",
      "        0.0203, 0.1464, 0.0943, 0.0242, 0.0326, 0.0140, 0.0340, 0.0149, 0.0053])\n",
      "label (actual next character): 13\n",
      "probability assigned by the the net tothe correct char: 0.0029116913210600615\n",
      "log likelihood: -5.8390212059021\n",
      "Negative log likelihood: 5.8390212059021\n",
      "----------------------------------------------------------------\n",
      "bigram example 4: ma (indexes 13,1)\n",
      "input to the neural net: 13\n",
      "output probabilities from the neural net: tensor([0.0123, 0.0078, 0.0250, 0.0187, 0.0050, 0.0125, 0.0074, 0.0099, 0.1254,\n",
      "        0.1147, 0.0934, 0.0176, 0.0144, 0.0029, 0.0085, 0.0116, 0.1132, 0.0138,\n",
      "        0.0203, 0.1464, 0.0943, 0.0242, 0.0326, 0.0140, 0.0340, 0.0149, 0.0053])\n",
      "label (actual next character): 1\n",
      "probability assigned by the the net tothe correct char: 0.007754605263471603\n",
      "log likelihood: -4.859468460083008\n",
      "Negative log likelihood: 4.859468460083008\n",
      "----------------------------------------------------------------\n",
      "bigram example 5: a. (indexes 1,0)\n",
      "input to the neural net: 1\n",
      "output probabilities from the neural net: tensor([0.0934, 0.0195, 0.0256, 0.0191, 0.0581, 0.0062, 0.0103, 0.0197, 0.1369,\n",
      "        0.0338, 0.0161, 0.0334, 0.0113, 0.0052, 0.0665, 0.0102, 0.0135, 0.0069,\n",
      "        0.2054, 0.0072, 0.0151, 0.0099, 0.0127, 0.0266, 0.0416, 0.0151, 0.0809])\n",
      "label (actual next character): 0\n",
      "probability assigned by the the net tothe correct char: 0.09339159727096558\n",
      "log likelihood: -2.3709537982940674\n",
      "Negative log likelihood: 2.3709537982940674\n",
      "=========\n",
      "average negative log likelihood or loss =  4.372129917144775\n"
     ]
    }
   ],
   "source": [
    "# probs will be the output, gives the probability of each char being next char\n",
    "# we want to optimise W to get correct probs(measure by loss function)\n",
    "nlls = torch.zeros(5) # negative log likelihood\n",
    "for i in range(5):\n",
    "    #ith bigram\n",
    "    x = xs[i].item() # input char index\n",
    "    y = ys[i].item() # output/label char index\n",
    "    print('----------------------------------------------------------------')\n",
    "    print(f'bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x},{y})')\n",
    "    print('input to the neural net:',x)\n",
    "    print('output probabilities from the neural net:', probs[i])\n",
    "    print('label (actual next character):', y)\n",
    "    p = probs[i,y]\n",
    "    print('probability assigned by the the net tothe correct char:', p.item())\n",
    "    logp = torch.log(p)\n",
    "    print('log likelihood:', logp.item())\n",
    "    nll = - logp\n",
    "    print('Negative log likelihood:', nll.item())\n",
    "    nlls[i]=nll\n",
    "print('=========')\n",
    "print('average negative log likelihood or loss = ', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTIMISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228146\n"
     ]
    }
   ],
   "source": [
    "# using complete dataset\n",
    "\n",
    "xs,ys =[],[]\n",
    "for w in words:\n",
    "    chs = ['.']+list(w)+['.']\n",
    "    for ch1, ch2 in zip(chs,chs[1:]):\n",
    "        ix1, ix2 = stoi[ch1],stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "        \n",
    "xs = torch.tensor(xs) #torch.Tensor(), capital T is different, torch.Tensor() return dtype as float32, lowercase infers the datatype (int in this case)\n",
    "ys = torch.tensor(ys)# recommended to always use torch.tensor and not torch.Tensor\n",
    "num= xs.nelement()\n",
    "print(num) # we are working with these many examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising weights\n",
    "g= torch.Generator().manual_seed(42)\n",
    "W = torch.randn((27,27),generator=g,requires_grad=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization - equivalent to model smoothing\n",
    "# alpha * (W**2).mean(), we can add this term to the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss after epoch 0 : 3.7064766883850098\n",
      "loss after epoch 1 : 3.149928331375122\n",
      "loss after epoch 2 : 2.9258015155792236\n",
      "loss after epoch 3 : 2.7928905487060547\n",
      "loss after epoch 4 : 2.7342138290405273\n",
      "loss after epoch 5 : 2.685577154159546\n",
      "loss after epoch 6 : 2.675067663192749\n",
      "loss after epoch 7 : 2.6397056579589844\n",
      "loss after epoch 8 : 2.6286978721618652\n",
      "loss after epoch 9 : 2.608739137649536\n",
      "loss after epoch 10 : 2.610383987426758\n",
      "loss after epoch 11 : 2.5883595943450928\n",
      "loss after epoch 12 : 2.5871224403381348\n",
      "loss after epoch 13 : 2.57504940032959\n",
      "loss after epoch 14 : 2.581500291824341\n",
      "loss after epoch 15 : 2.5633888244628906\n",
      "loss after epoch 16 : 2.565204381942749\n",
      "loss after epoch 17 : 2.556692600250244\n",
      "loss after epoch 18 : 2.565431594848633\n",
      "loss after epoch 19 : 2.5485620498657227\n",
      "loss after epoch 20 : 2.5514988899230957\n",
      "loss after epoch 21 : 2.545212745666504\n",
      "loss after epoch 22 : 2.5555033683776855\n",
      "loss after epoch 23 : 2.5388410091400146\n",
      "loss after epoch 24 : 2.5420901775360107\n",
      "loss after epoch 25 : 2.5375595092773438\n",
      "loss after epoch 26 : 2.5490715503692627\n",
      "loss after epoch 27 : 2.5321125984191895\n",
      "loss after epoch 28 : 2.535227060317993\n",
      "loss after epoch 29 : 2.5322375297546387\n",
      "loss after epoch 30 : 2.54477596282959\n",
      "loss after epoch 31 : 2.527255058288574\n",
      "loss after epoch 32 : 2.5299811363220215\n",
      "loss after epoch 33 : 2.5283820629119873\n",
      "loss after epoch 34 : 2.541808605194092\n",
      "loss after epoch 35 : 2.523620367050171\n",
      "loss after epoch 36 : 2.5258443355560303\n",
      "loss after epoch 37 : 2.5254712104797363\n",
      "loss after epoch 38 : 2.539658308029175\n",
      "loss after epoch 39 : 2.52082896232605\n",
      "loss after epoch 40 : 2.522543430328369\n",
      "loss after epoch 41 : 2.5231902599334717\n",
      "loss after epoch 42 : 2.5379984378814697\n",
      "loss after epoch 43 : 2.5186474323272705\n",
      "loss after epoch 44 : 2.5199222564697266\n",
      "loss after epoch 45 : 2.5213515758514404\n",
      "loss after epoch 46 : 2.5366342067718506\n",
      "loss after epoch 47 : 2.516918182373047\n",
      "loss after epoch 48 : 2.517862558364868\n",
      "loss after epoch 49 : 2.5198423862457275\n",
      "loss after epoch 50 : 2.535463809967041\n",
      "loss after epoch 51 : 2.5155277252197266\n",
      "loss after epoch 52 : 2.516252279281616\n",
      "loss after epoch 53 : 2.5185909271240234\n",
      "loss after epoch 54 : 2.5344393253326416\n",
      "loss after epoch 55 : 2.5143911838531494\n",
      "loss after epoch 56 : 2.514986991882324\n",
      "loss after epoch 57 : 2.5175464153289795\n",
      "loss after epoch 58 : 2.533540964126587\n",
      "loss after epoch 59 : 2.5134475231170654\n",
      "loss after epoch 60 : 2.5139758586883545\n",
      "loss after epoch 61 : 2.516669273376465\n",
      "loss after epoch 62 : 2.5327558517456055\n",
      "loss after epoch 63 : 2.5126516819000244\n",
      "loss after epoch 64 : 2.513151168823242\n",
      "loss after epoch 65 : 2.515927314758301\n",
      "loss after epoch 66 : 2.532072067260742\n",
      "loss after epoch 67 : 2.5119740962982178\n",
      "loss after epoch 68 : 2.512465000152588\n",
      "loss after epoch 69 : 2.515294075012207\n",
      "loss after epoch 70 : 2.5314764976501465\n",
      "loss after epoch 71 : 2.5113916397094727\n",
      "loss after epoch 72 : 2.5118837356567383\n",
      "loss after epoch 73 : 2.514749765396118\n",
      "loss after epoch 74 : 2.530958414077759\n",
      "loss after epoch 75 : 2.5108871459960938\n",
      "loss after epoch 76 : 2.5113844871520996\n",
      "loss after epoch 77 : 2.514277219772339\n",
      "loss after epoch 78 : 2.5305049419403076\n",
      "loss after epoch 79 : 2.5104479789733887\n",
      "loss after epoch 80 : 2.5109522342681885\n",
      "loss after epoch 81 : 2.5138657093048096\n",
      "loss after epoch 82 : 2.5301074981689453\n",
      "loss after epoch 83 : 2.5100629329681396\n",
      "loss after epoch 84 : 2.5105743408203125\n",
      "loss after epoch 85 : 2.51350474357605\n",
      "loss after epoch 86 : 2.5297579765319824\n",
      "loss after epoch 87 : 2.509723424911499\n",
      "loss after epoch 88 : 2.5102415084838867\n",
      "loss after epoch 89 : 2.513185977935791\n",
      "loss after epoch 90 : 2.5294487476348877\n",
      "loss after epoch 91 : 2.5094237327575684\n",
      "loss after epoch 92 : 2.5099470615386963\n",
      "loss after epoch 93 : 2.5129036903381348\n",
      "loss after epoch 94 : 2.529174327850342\n",
      "loss after epoch 95 : 2.5091569423675537\n",
      "loss after epoch 96 : 2.5096848011016846\n",
      "loss after epoch 97 : 2.5126523971557617\n",
      "loss after epoch 98 : 2.5289294719696045\n",
      "loss after epoch 99 : 2.5089192390441895\n"
     ]
    }
   ],
   "source": [
    "# since we only have one parameter,W we dont need to call .parameters method and loop over it\n",
    "lr = 100 # we are using a very large learning rate\n",
    "epochs = 100 \n",
    "for epoch in range(epochs):\n",
    "    #forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # now these vectors can be put in the NN, we want dtype to be float and not int\n",
    "    logits = xenc @ W # predict log counts\n",
    "    # applying softmax to output the probability distributions\n",
    "    counts = logits.exp() # equilvalent to N used in previous notebook\n",
    "    probs = counts/counts.sum(1,keepdim=True)\n",
    "    loss  = - probs[torch.arange(num),ys].log().mean() + 0.01 * (W**2).mean() # second term is regularization, it tries to penalise large W and tries to get it towards 0\n",
    "    print(f'loss after epoch {epoch} : {loss}')\n",
    "    # backward pass\n",
    "    W.grad = None # a way to set gradients to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    #update params\n",
    "    W.data += -lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.50962495803833\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference, sampling from neural net model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ren.\n",
      "roinyaclynenelaza.\n",
      "jerekobaien.\n",
      "kiah.\n",
      "kosa.\n",
      "paf.\n",
      "jerinirumuaydowon.\n",
      "jea.\n",
      "mber.\n",
      "r.\n",
      "zorl.\n",
      "siliopxalon.\n",
      "wrifma.\n",
      "caciowelen.\n",
      "yn.\n",
      "mi.\n",
      "ly.\n",
      "n.\n",
      "sik.\n",
      "pera.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(43)\n",
    "for i in range(20):\n",
    "    out =[]\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict log counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts/counts.sum(1, keepdim=True) # probabilities for next character\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1,replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used a single linear layer to create the neural net, we didnt use any bias term\n",
    "# ideally this can get as good as the bigram model implemented using counts, it cant surpass that\n",
    "# coz we counted the actual probabilities there and here we tried to arrive at those probs by optimising the weights\n",
    "\n",
    "# cons we only took one previous character as context- so it doesnt give good predictions\n",
    "# next we will look to look at chars before and after for context maybe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
