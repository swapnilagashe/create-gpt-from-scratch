{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "number of characters in dataset: 1115394 \n",
      "\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "input_data_path = 'data/input.txt'\n",
    "with open(input_data_path, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "print(type(text))\n",
    "print('number of characters in dataset:', len(text),'\\n\\n')\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "vocab_chars = sorted(list(set(text)))\n",
    "vocab_size = len(vocab_chars)\n",
    "print(vocab_chars)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_str: Hello World!\n",
      "encoded_str: [20, 43, 50, 50, 53, 1, 35, 53, 56, 50, 42, 2]\n",
      "decoded_str: Hello World!\n"
     ]
    }
   ],
   "source": [
    "# create a mapping between characters and integers using our vocab list\n",
    "str_to_i = {char:i for i,char in enumerate(vocab_chars)}\n",
    "i_to_str = {i:char for i,char in enumerate(vocab_chars)}\n",
    "\n",
    "# define character level encoder, to convert chars in integers\n",
    "encode = lambda s : [str_to_i[c] for c in s] \n",
    "# define decoder, to convert integers to chars\n",
    "decode = lambda s : ''.join([i_to_str[c] for c in s] )\n",
    "\n",
    "sample_str = 'Hello World!'\n",
    "encoded_str = encode(sample_str)\n",
    "decoded_str = decode(encoded_str)\n",
    "print(f'sample_str: {sample_str}\\nencoded_str: {encoded_str}\\ndecoded_str: {decoded_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_to_i['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize complete data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train and val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frac = 0.9 # using 90 percent of data for training and remaining data for validation\n",
    "n  = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" Note - 1. we never feed the whole text into the transformer all at once as its not computationally efficient.\n",
    "2. instead we break it down into chunks of text and sample random loads of the chunks to feed to the transformer at a time\n",
    "3. In the following example we are having block size of 8 but 9 chars are taken because we want to have 8 samples for transformer to predict (n-1)\"\"\"\n",
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is the context:tensor([18]), the target is: 47\n",
      "when input is the context:tensor([18, 47]), the target is: 56\n",
      "when input is the context:tensor([18, 47, 56]), the target is: 57\n",
      "when input is the context:tensor([18, 47, 56, 57]), the target is: 58\n",
      "when input is the context:tensor([18, 47, 56, 57, 58]), the target is: 1\n",
      "when input is the context:tensor([18, 47, 56, 57, 58,  1]), the target is: 15\n",
      "when input is the context:tensor([18, 47, 56, 57, 58,  1, 15]), the target is: 47\n",
      "when input is the context:tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1] # off set by one coz this is the set that we want to predict eg, for x[0]->x[1] or y[0] is the pred and so on\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input is the context:{context}, the target is: {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 4 # number of independent sequences to process parallely\n",
    "block_size = 8 # maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate small batch of data of inputs x, and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size] for i in ix])\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[ 6,  0, 14, 43, 44, 53, 56, 43],\n",
      "        [39,  1, 42, 59, 43,  1, 39, 52],\n",
      "        [47, 41, 43,  1, 39, 52, 42,  1],\n",
      "        [53, 44,  1, 50, 43, 58,  1, 58]])\n",
      "targets:\n",
      "torch.Size([4, 7])\n",
      "tensor([[ 0, 14, 43, 44, 53, 56, 43],\n",
      "        [ 1, 42, 59, 43,  1, 39, 52],\n",
      "        [41, 43,  1, 39, 52, 42,  1],\n",
      "        [44,  1, 50, 43, 58,  1, 58]])\n"
     ]
    }
   ],
   "source": [
    "xb,yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 65])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn import functional as f\n",
    "torch.manual_seed(42)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "        \n",
    "    def forward(self,idx,targets):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits  = self.token_embedding_table(idx) #(B,T,C)\n",
    "        loss = F.cross_entropy(logits,targets)\n",
    "        return logits\n",
    "    \n",
    "m = BigramLanguageModel(vocab_size)\n",
    "out = m(xb,yb)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
